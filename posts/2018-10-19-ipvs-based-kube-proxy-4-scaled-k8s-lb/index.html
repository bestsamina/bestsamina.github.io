<!DOCTYPE html>
<html lang="en-us">
    <head>
    <link href="http://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.44" />
    
    
    <title>IPVS-based Kube-proxy for Scaled Kubernetes Load Balancing • Samina&#39;s 技術筆記 X 點滴紀錄</title>
<meta name="description" content="Samina&#39;s 技術筆記 X 點滴紀錄 :: 記錄一些技術筆記與生活紀錄">
<meta name="keywords" content="blog,python,golang,javascript,container,opencord,cord,sdn,network">
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="IPVS-based Kube-proxy for Scaled Kubernetes Load Balancing"/>
<meta name="twitter:description" content="這篇為 10月19日 talk 的文字版， slides 是 https://speakerdeck.com/sufuf3/ipvs-based-kube-proxy-for-scaled-kubernetes-load-balancing 。
 內容脈絡
 Preface (前言) Introduction (介紹) Kube-Proxy  What is Kube-proxy (什麼是 Kube-proxy) Kube-Proxy mode  IPVS  LVS What is IPVS (什麼是 IPVS) IPVS with Netfilter (IPVS 和 Netfilter) IPVS vs iptables (IPVS 與 iptables 的比較)  IPVS-based Kube-proxy  Why using IPVS? (為什麼要用 IPVS) How IPVS-based Kube-proxy work? (IPVS-based Kube-proxy 是怎麼運作的) Run Kube-proxy in IPVS mode (來執行 IPVS mode 的 Kube-proxy) IPVS Service Network Topology Example  Implement IPVS-based K8s service load balancing (實現 IPVS-based K8s service load balancing) Conclusion (結論)  Preface (前言) 在一般使用 Kubernetes 的 kube-proxy 情況下，通常都使用 iptables 模式。"/>

<meta property="og:title" content="IPVS-based Kube-proxy for Scaled Kubernetes Load Balancing" />
<meta property="og:description" content="這篇為 10月19日 talk 的文字版， slides 是 https://speakerdeck.com/sufuf3/ipvs-based-kube-proxy-for-scaled-kubernetes-load-balancing 。
 內容脈絡
 Preface (前言) Introduction (介紹) Kube-Proxy  What is Kube-proxy (什麼是 Kube-proxy) Kube-Proxy mode  IPVS  LVS What is IPVS (什麼是 IPVS) IPVS with Netfilter (IPVS 和 Netfilter) IPVS vs iptables (IPVS 與 iptables 的比較)  IPVS-based Kube-proxy  Why using IPVS? (為什麼要用 IPVS) How IPVS-based Kube-proxy work? (IPVS-based Kube-proxy 是怎麼運作的) Run Kube-proxy in IPVS mode (來執行 IPVS mode 的 Kube-proxy) IPVS Service Network Topology Example  Implement IPVS-based K8s service load balancing (實現 IPVS-based K8s service load balancing) Conclusion (結論)  Preface (前言) 在一般使用 Kubernetes 的 kube-proxy 情況下，通常都使用 iptables 模式。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bestsamina.github.io/posts/2018-10-19-ipvs-based-kube-proxy-4-scaled-k8s-lb/" />



<meta property="article:published_time" content="2018-10-15T12:05:37&#43;08:00"/>

<meta property="article:modified_time" content="2018-10-15T12:05:37&#43;08:00"/>












    <!-- Font-Awesome -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/solid.css" integrity="sha384-TbilV5Lbhlwdyc4RuIV/JhD8NR+BfMrvz4BL5QFa2we1hQu6wvREr3v6XSRfCTRp" crossorigin="anonymous">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/brands.css" integrity="sha384-7xAnn7Zm3QC1jFjVc1A6v/toepoG3JXboQYzbM0jrPzou9OFXm/fY6Z/XiIebl/k" crossorigin="anonymous">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/fontawesome.css" integrity="sha384-ozJwkrqb90Oa3ZNb+yKFW2lToAWYdTiF1vt8JiH5ptTGHTGcN7qdoR1F95e0kYyG" crossorigin="anonymous">
    <!-- highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <link rel="stylesheet" href="https://bestsamina.github.io//css/print.min.css" media="print">
    <link rel="stylesheet" href="https://bestsamina.github.io//css/hyde-hyde.css">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png"></head>

    <body>
        
<div class="sidebar text-center">
  <div class="container ">
    <div class="sidebar-about">
      <div class="site__title">
        <a href="https://bestsamina.github.io/">Samina&#39;s 技術筆記 X 點滴紀錄</a>
      </div>
      
      
      
      <p>
        <img src="https://bestsamina.github.io/images/sufuf3_1519028337_878.jpg" alt="Author Image" class="img--circle img--headshot element--center"> 
      </p>
      
      <p class="site__description">
         記錄一些技術筆記與生活紀錄 
      </p>
    </div>
    <div>
	<ul class="sidebar-nav">
		
				<li>
					<a href="/posts/">
						<span>Posts</span>
					</a>
				</li>
				<li>
					<a href="/series/">
						<span>Series</span>
					</a>
				</li>
				<li>
					<a href="/tags/">
						<span>Tags</span>
					</a>
				</li>
				<li>
					<a href="/categories/">
						<span>Categories</span>
					</a>
				</li>
				<li>
					<a href="/page/awesome-blogs">
						<span>Awesome-Blogs</span>
					</a>
				</li>
				<li>
					<a href="/about/">
						<span>About</span>
					</a>
				</li>
		</li>
	</ul>
</div>

    <p>
      <section class="social text-center">
	<a href="https://twitter.com/sufuf3149"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a>
	
	&nbsp;<a href="https://github.com/sufuf3"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	&nbsp;<a href="https://gitlab.com/sufuf3"><i class="fab fa-gitlab fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
</section>

    </p>
    <p class="copyright">
      &copy; 2018 Samina Fu.
      <a href="https://creativecommons.org/licenses/by-sa/4.0">Some Rights Reserved</a>.
      <br/>Built with
      <a href="https://gohugo.io">Hugo</a> ❤️ <a href="https://github.com/htr3n/hyde-hyde">hyde-hyde</a>.
      
    </p>
  </div>
  <div>
  </div>
</div>
<div class="content container">
            <article>
  <header>
    <h1>IPVS-based Kube-proxy for Scaled Kubernetes Load Balancing</h1>
     
    
<div class="post__meta">
    
    <i class="fas fa-calendar-alt"></i> Oct 15, 2018
    in
              
              
              <a class="post__category" href="/categories/kubernetes">A KUBERNETES</a>
              
              
    
      
      
          <br/>
           <i class="fas fa-tags"></i>
          
          <a class="post__tag" href="/tags/kubernetes">#kubernetes</a>
           
      
          <a class="post__tag" href="/tags/k8s">#k8s</a>
           
      
          <a class="post__tag" href="/tags/container">#container</a>
           
      
          <a class="post__tag" href="/tags/kube-proxy">#kube-proxy</a>
           
      
          <a class="post__tag" href="/tags/ipvs">#ipvs</a>
          
      
    
    
    <br/>
    <i class="fas fa-clock"></i> 6 min read
</div>


  </header>
  <div class="post">
    

<blockquote>
<p>這篇為 <a href="https://www.accupass.com/event/1810120759471826477649">10月19日 talk</a> 的文字版， slides 是 <a href="https://speakerdeck.com/sufuf3/ipvs-based-kube-proxy-for-scaled-kubernetes-load-balancing">https://speakerdeck.com/sufuf3/ipvs-based-kube-proxy-for-scaled-kubernetes-load-balancing</a> 。</p>
</blockquote>

<p>內容脈絡</p>

<ul>
<li>Preface (前言)</li>
<li>Introduction (介紹)</li>
<li>Kube-Proxy

<ul>
<li>What is Kube-proxy (什麼是 Kube-proxy)</li>
<li>Kube-Proxy mode</li>
</ul></li>
<li>IPVS

<ul>
<li>LVS</li>
<li>What is IPVS (什麼是 IPVS)</li>
<li>IPVS with Netfilter (IPVS 和 Netfilter)</li>
<li>IPVS vs iptables (IPVS 與 iptables 的比較)</li>
</ul></li>
<li>IPVS-based Kube-proxy

<ul>
<li>Why using IPVS? (為什麼要用 IPVS)</li>
<li>How IPVS-based Kube-proxy work? (IPVS-based Kube-proxy 是怎麼運作的)</li>
<li>Run Kube-proxy in IPVS mode (來執行 IPVS mode 的 Kube-proxy)</li>
<li>IPVS Service Network Topology</li>
<li>Example</li>
</ul></li>
<li>Implement IPVS-based K8s service load balancing (實現 IPVS-based K8s service load balancing)</li>
<li>Conclusion (結論)</li>
</ul>

<h2 id="preface-前言">Preface (前言)</h2>

<p>在一般使用 Kubernetes 的 kube-proxy 情況下，通常都使用 iptables 模式。<br />
然而在 2015/11/19 ，K8s 團隊的成員 - <a href="https://github.com/thockin">Tim Hockin</a> 提出了一個 Issue - <a href="https://github.com/kubernetes/kubernetes/issues/17470">&ldquo;Try kube-proxy via ipvs instead of iptables or userspace&rdquo;</a> 而底下也出現了一些討論。<br />
有趣的是，大家都說 IPVS 比 iptables 好。<br />
在去年，華為提出了使用 IPVS 優化 service 性能的場景。(<a href="https://github.com/kubernetes/kubernetes/issues/44063">ref1</a>, <a href="https://zhuanlan.zhihu.com/p/37230013">ref2</a>)<br />
今年 2018/06/28 <a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.11.0">K8s v1.11</a> 也新增了 IPVS-based kube-proxy 的功能。(<a href="https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/">ref1</a>, <a href="https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/">ref2</a>)</p>

<p>也因此，本篇就來探討</p>

<ol>
<li>到底 IPVS 是什麼</li>
<li>為什麼 kube-proxy 使用 IPVS mode 會比 iptables mode 好</li>
<li>kube-proxy 的 IPVS 是怎麼實現的</li>
<li>要怎麼使用 kube-proxy 的 IPVS mode 來實現 Kubernetes service 的 load balance</li>
</ol>

<h2 id="introduction-介紹">Introduction (介紹)</h2>

<p>在本篇，將介紹 IPVS 與 Kube-proxy 的 IPVS mode 。並且說明 IPVS 與 iptable 的差異。以及實作。</p>

<h2 id="kube-proxy">Kube-Proxy</h2>

<h3 id="what-is-kube-proxy-什麼是-kube-proxy">What is Kube-proxy (什麼是 Kube-proxy)</h3>

<p><a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/"><img src="https://i.imgur.com/apjNBut.png" alt="" /></a><br />
在官網中的這張圖，會看到每一台 Minion node 都有 kube-proxy 這個元件。<br />
所以 kube-proxy 是什麼樣的功能要長在每一台 node 上呢？<br />
要談論 kube-proxy 就不得不和 service 一起談論了。</p>

<p>我們知道當 backend 的 pod 需要提供 port 給外部 access 時候，就需要 service 。<br />
Service 是抽象的一層，主要定義一組 pod 和網路的規則讓外部可以 access 。<br />
那 service 定義好這些網路的訪問規則，那總要有人來實現這些規則，這就要提到 kube-proxy 了。<br />
因為 kube-proxy 是實現 service 這功能的關鍵。</p>

<p><img src="https://d33wubrfki0l68.cloudfront.net/cc38b0f3c0fd94e66495e3a4198f2096cdecd3d5/ace10/docs/tutorials/kubernetes-basics/public/images/module_04_services.svg" alt="" /></p>

<p>kube-proxy 介紹：</p>

<ul>
<li>管理 host 上的網路規則來確保 k8s 的 service 定義的規則</li>
<li>執行連線的 forwarding</li>
<li>在每一台 node 上都有 kube-proxy</li>
<li>proxies UDP, TCP and SCTP</li>
<li>提供 load balancing</li>
<li>專門實現 service
<br /></li>
</ul>

<p>那要怎麼達到管理 host 上的網路規則，這就要來看他有提供哪些 mode。</p>

<h3 id="kube-proxy-mode">Kube-Proxy mode</h3>

<p>有<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/">三種</a>：</p>

<ul>
<li><p><a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/proxy/userspace">userspace (older)</a>:
從圖片中我們可以看到，當外部 Client 請求 service IP 後，會比對 iptales 的 rule 然後將封包轉送到 kube-Proxy 這個 app pod ，再由 kube-proxy 處理封包的轉送到後端的 pod。<br />
<img src="https://d33wubrfki0l68.cloudfront.net/b8e1022c2dd815d8dd36b1bc4f0cc3ad870a924f/1dd12/images/docs/services-userspace-overview.svg" alt="" /></p></li>

<li><p><a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/proxy/iptables">iptables (faster)</a>:
iptables 就比 userspace 好一些，因為 client 請求 service IP 後，就交給 iptables 處理封包的轉送。而下 iptables rule 就是當 api-server 拿到 service 的 yaml 檔後，api-server 會給 kube-proxy 來處理，而 kube-proxy 就來下 iptables 的 rule 囉！<br />
<img src="https://d33wubrfki0l68.cloudfront.net/837afa5715eb31fb9ca6516ec6863e810f437264/42951/images/docs/services-iptables-overview.svg" alt="" /></p></li>

<li><p><a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/proxy/ipvs">ipvs (experimental)</a>:
和 iptables mode 方式雷同，但是是使用 kernel space 的 IPVS 方法處理要轉送的封包，不受限於 iptables 的規則。<br />
<img src="https://d33wubrfki0l68.cloudfront.net/3ece58e40119539d5e999fe137e79b5015c24275/77563/images/docs/services-ipvs-overview.svg" alt="" /></p></li>
</ul>

<h2 id="ipvs">IPVS</h2>

<p>在講 IPVS 之前，我們不得不提到 LVS。</p>

<h3 id="lvs">LVS</h3>

<p>LVS 全名是 <code>Linux Virtual Server</code>。它是在 cloud 上或是真實 server 上的高可擴展與高可用的 server。通常會伴隨 load balancer 一起在 Linux OS 上運行。<br />
最有名的就是這張圖片拉！<br />
<img src="https://i.imgur.com/9vLMHjv.png" alt="" /><br />
對外面的 user 來說，他 access 的是個機器，但對集群來說 user access 到的其實是虛擬的 server，而虛擬的 server 是由一群在同個 LAN 或是 WAN 中的真實的 server 和 Load Balancer 所組成的。</p>

<p>那 Virtual Server 總是要有個 IP 給外面的 user 知道吧！所以<a href="http://www.linuxvirtualserver.org/HighAvailability.html">官方網站</a>也提供下面這張圖，由圖所知，就是會有個 Virtual IP 拉！<br />
<img src="https://i.imgur.com/EU0gAUv.png" alt="" /></p>

<p>那 LVS 和 IPVS 的關係又是什麼呢？<br />
這要來看看 <a href="http://www.linuxvirtualserver.org/about.html">LVS 的框架</a>了。<br />
<img src="https://i.imgur.com/lE6iI9F.png" alt="" /><br />
由圖中所知，IPVS 是在 LVS 框架中最底層的位置。也就是說 LVS 是 base on IPVS 來實現的拉！那我們就要來看看 IPVS 了。</p>

<h3 id="what-is-ipvs-什麼是-ipvs">What is IPVS (什麼是 IPVS)</h3>

<ul>
<li>IPVS 全名 IP Virtual Server</li>
<li>實現 L4 的 load balancing</li>
<li>也稱 Layer-4 switching</li>
<li>主要在 cluster 前的實體主機上運行</li>
<li>直接請求 base on service 的 TCP / UDP 到真實的 server</li>
<li>使真實 server 的 service 顯示為有一個 IP 的虛擬 service</li>
<li>實現為 Netfilter 框架上的 module</li>
<li>Based on kernel 中的 hash tables</li>
<li>Kernel source code: net/netfilter/ipvs</li>
<li>ipvsadm 是管理 IPVS 的工具</li>
</ul>

<h3 id="ipvs-with-netfilter-ipvs-和-netfilter">IPVS with Netfilter (IPVS 和 Netfilter)</h3>

<p>那我們知道 IPVS 是使用 Netfilter 的 module，那就來看看 IPVS 和 Netfilter 之間的關係吧！<br />
&gt; Ref: <a href="http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.filter_rules.html">http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.filter_rules.html</a>, <a href="https://www.digitalocean.com/community/tutorials/a-deep-dive-into-iptables-and-netfilter-architecture">https://www.digitalocean.com/community/tutorials/a-deep-dive-into-iptables-and-netfilter-architecture</a>, <a href="http://www.178linux.com/13570">http://www.178linux.com/13570</a></p>

<p><img src="https://i.imgur.com/i60QKw4.png" alt="" /></p>

<ol>
<li>當 client 的請求從外部進到 Load balencer 的 kernel space 時，會先到 PREROUTING 的 chain。</li>
<li>接著 Route 會依據進來的封包 destination address 判斷是不是本機的(通常是 VIP 存在於本機的 network interface 上)。如果是本機的封包，就將封包往 INPUT chain 送。</li>
<li>而 ip_vs_in 掛勾到 LOCAL_IN，所以最終會給 IPVS 來檢查這個進來的封包。
(module 以優先權來決定，優先權低的先看封包，IPVS 的優先權高於 iptables 所以會先給 iptable 看，之後給 IPVS 看。)</li>
<li>在 LOCAL_IN Hook 如果 ip_vs_in 發現這個封包都有在 IPVS 規則中，就會 trigger INPUT chain 之後到 POSTROUTING chain 就結束。
由上面可知，使用 IPVS 不會接觸到 iptables 的 rule。</li>
</ol>

<h3 id="ipvs-vs-iptables-ipvs-與-iptables-的比較">IPVS vs iptables (IPVS 與 iptables 的比較)</h3>

<p>大致了解 IPVS ，那 IPVS 和 iptables 有什麼差別呢？<br />
他們其實都是使用 Netfilter ，來讓封包達到轉送的機制。<br />
但實作面卻是不一樣的。<br />
在 INPUT chain 這邊，不論是 IPVS 或是 iptables 都會到 userspace 來進行封包解析，來看看封包要往哪邊走。<br />
然而，就是在判斷封包要往哪邊走，這邊的方法兩者使用的方式是不一樣的。<br />
iptables 規則設定是：n 張 table，每張 table 內有 m 個 chain ，每個 chain 中有 rule。如圖(source: <a href="https://www.thegeekstuff.com/2011/01/iptables-fundamentals/">https://www.thegeekstuff.com/2011/01/iptables-fundamentals/</a>)<br />
<img src="https://static.thegeekstuff.com/wp-content/uploads/2011/01/iptables-table-chain-rule-structure.png" alt="" /><br />
雖然封包不會跑過所有的 Table 和 chain。<br />
但 rules 通常會是很多的。雖然封包只要被拆解一次，但封包在比對每個 rule 時，都要再看要用進來的這個封包用什麼欄位來和目前輪到的 rule 進行比對。<br />
這也是為什麼我們在下 rule 的時候很重視 rule 的順序性。(所以 rule 也會因人的設定好不好，效能也會有差異)<br />
IPVS 在判斷上面就簡單很多，他是用 hash table(雜湊表)，在時間複雜度上，是 O(n)。理論可以參考：<a href="https://zh.wikipedia.org/wiki/%E5%93%88%E5%B8%8C%E8%A1%A8">https://zh.wikipedia.org/wiki/%E5%93%88%E5%B8%8C%E8%A1%A8</a></p>

<p>就上面的兩種分析封包判斷要往哪走的方法，我們也可以了解到他們在封包轉送上的效率是有差別的。</p>

<p>而華為在去年的<a href="https://www.slideshare.net/LCChina/scale-kubernetes-to-support-50000-services">演講</a>中，也有提供 k8s kube-proxy 中使用 iptables 和 IPVS mode 的差異。(From 華為投影片)<br />
<img src="https://i.imgur.com/HrW0m0z.png" alt="" />
<img src="https://i.imgur.com/uUFNSMa.png" alt="" /></p>

<h2 id="ipvs-based-kube-proxy">IPVS-based Kube-proxy</h2>

<h3 id="why-using-ipvs-為什麼要用-ipvs">Why using IPVS? (為什麼要用 IPVS)</h3>

<p>承接上面，我們可以得出為什麼要使用 IPVS 的結論(<a href="https://www.cncf.io/wp-content/uploads/2018/08/CNCF-Webinar_-Kubernetes-1.11-1.pdf">ref</a>)。</p>

<ol>
<li>有較佳的 performance (Hashing 和 Chain 的效率)</li>
<li>有比較多的 load balancing 演算法(多達 8 種)</li>
<li>支援 server 運行狀況檢查和連接重試</li>
<li>支援 sticky session</li>
<li>iptables 操作在大規模集群中顯得比較慢</li>
</ol>

<h3 id="how-ipvs-based-kube-proxy-work-ipvs-based-kube-proxy-是怎麼運作的">How IPVS-based Kube-proxy work? (IPVS-based Kube-proxy 是怎麼運作的)</h3>

<p>那是怎麼運作的呢？我們回頭再來看這張圖片<br />
<img src="https://d33wubrfki0l68.cloudfront.net/3ece58e40119539d5e999fe137e79b5015c24275/77563/images/docs/services-ipvs-overview.svg" alt="" /><br />
我們知道在 Virtual Server 這邊是使用 IPVS ，然後透過他管理的 virtual server table 來讓 Netfilter 把封包送到對的地方。<br />
這就是他的運作原理囉！</p>

<h3 id="run-kube-proxy-in-ipvs-mode-來執行-ipvs-mode-的-kube-proxy">Run Kube-proxy in IPVS mode (來執行 IPVS mode 的 Kube-proxy)</h3>

<p>那在 kube-proxy 中我們要怎麼使用 IPVS mode 呢？(<a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/proxy/ipvs">ref</a>)</p>

<p><strong>1. 首先我們要把 IPVS 的 kernel module load 進來</strong></p>

<pre><code>modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
cut -f1 -d &quot; &quot;  /proc/modules | grep -e ip_vs -e nf_conntrack_ipv4
</code></pre>

<p><strong>2. 接著在啟動 kube-proxy 時，參數設為</strong></p>

<pre><code>--proxy-mode=ipvs
</code></pre>

<p>(如果要使用其他演算法，那可以設定 <code>--ipvs-scheduler=rr</code> rr 改為其他的)</p>

<p><strong>3. 如果是在 v1.10 之前的版本， kube-proxy 要加下面的參數</strong></p>

<pre><code>--feature-gates=SupportIPVSProxyMode=true
</code></pre>

<h3 id="ipvs-service-network-topology">IPVS Service Network Topology</h3>

<p>當建立 ClusterIP type 的 service 時，IPVS proxier 將執行以下三項操作：</p>

<ul>
<li>確保節點中存在 dummy 接口，默認為 kube-ipvs0</li>
<li>將 service IP 綁到 dummy 接口</li>
<li>分別為每個 service IP 建立 IPVS virtual servers</li>
</ul>

<h3 id="example">Example</h3>

<ul>
<li>v1.10.x</li>
</ul>

<pre><code># kubectl describe svc nginx -n a-ns
Name:              nginx
Namespace:         a-ns
Labels:            run=nginx
Annotations:       &lt;none&gt;
Selector:          run=nginx
Type:              ClusterIP
IP:                10.105.12.124
External IPs:      100.67.151.9
Port:              &lt;unset&gt;  80/TCP
TargetPort:        80/TCP
Endpoints:         10.244.241.156:80,10.244.241.158:80
Session Affinity:  None
Events:            &lt;none&gt;

# ip a
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:26:2d:08:03:a4 brd ff:ff:ff:ff:ff:ff
    inet 100.67.151.2/16 brd 100.67.255.255 scope global noprefixroute eth0
       valid_lft forever preferred_lft forever
    inet 100.67.151.9/16 brd 100.67.255.255 scope global secondary eth0:1
       valid_lft forever preferred_lft forever
18: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default
    link/ether e6:f5:f6:9f:0b:9a brd ff:ff:ff:ff:ff:ff
    inet 10.96.0.1/32 brd 10.96.0.1 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.96.0.10/32 brd 10.96.0.10 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.105.12.124/32 brd 10.105.12.124 scope global kube-ipvs0
       valid_lft forever preferred_lft forever

# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.105.12.124:80 rr
  -&gt; 10.244.241.156:80            Masq    1      0          0
  -&gt; 10.244.241.158:80            Masq    1      0          0
TCP  100.67.151.9:80 rr
  -&gt; 10.244.241.156:80            Masq    1      0          0
  -&gt; 10.244.241.158:80            Masq    1      0          0
</code></pre>

<ul>
<li>v1.11.x</li>
</ul>

<pre><code># kubectl describe svc nginx -n a-ns
Name:              nginx
Namespace:         a-ns
Labels:            run=nginx
Annotations:       &lt;none&gt;
Selector:          run=nginx
Type:              ClusterIP
IP:                10.105.12.124
External IPs:      100.67.151.9
Port:              &lt;unset&gt;  80/TCP
TargetPort:        80/TCP
Endpoints:         10.244.241.156:80,10.244.241.158:80
Session Affinity:  None
Events:            &lt;none&gt;

# ip a
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:26:2d:08:03:a4 brd ff:ff:ff:ff:ff:ff
    inet 100.67.151.2/16 brd 100.67.255.255 scope global noprefixroute eth0
       valid_lft forever preferred_lft forever
18: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default
    link/ether e6:f5:f6:9f:0b:9a brd ff:ff:ff:ff:ff:ff
    inet 10.96.0.1/32 brd 10.96.0.1 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.96.0.10/32 brd 10.96.0.10 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.105.12.124/32 brd 10.105.12.124 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 100.67.151.9/16 brd 100.67.255.255 scope global kube-ipvs0
       valid_lft forever preferred_lft forever

# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.105.12.124:80 rr
  -&gt; 10.244.241.156:80            Masq    1      0          0
  -&gt; 10.244.241.158:80            Masq    1      0          0
TCP  100.67.151.9:80 rr
  -&gt; 10.244.241.156:80            Masq    1      0          0
  -&gt; 10.244.241.158:80            Masq    1      0          0
</code></pre>

<h2 id="implement-ipvs-based-k8s-service-load-balancing-實現-ipvs-based-k8s-service-load-balancing">Implement IPVS-based K8s service load balancing (實現 IPVS-based K8s service load balancing)</h2>

<p>請參考這篇<a href="https://bestsamina.github.io/posts/2018-10-15-hands-on-k8s-kube-proxy-w-ipvs-lb/">筆記</a></p>

<h2 id="conclusion-結論">Conclusion (結論)</h2>

<ul>
<li>IPVS 是 LVS 中的 L4 負載均衡器</li>
<li>IPVS提供

<ul>
<li>在 cluster 中更好的可擴展性和效能</li>
<li>有比 iptables 更多的 load balancing 演算法</li>
<li>server 的健康檢查和連接重試等</li>
</ul></li>
<li>我們可以使用 kube-proxy 的 IPVS 模式</li>
<li>了解 kube-proxy 的 IPVS mode 的運作原理</li>
</ul>

<h2 id="其他">其他</h2>

<p>在搜尋 IPVS 的過程中，發現了 <a href="https://lwn.net/Articles/747551/">BPF(Berkeley Packet Filter)</a> 一個即將要取代 iptables 的工具。 cilium 這篇 blog 也指出為何要用 BPF。 而 Facebook 也發現 BPF 效能比 IPVS 好，所以換成 BPF。<br />
有時間看來要研究一下。</p>

<ul>
<li>Blog <a href="https://cilium.io/blog/2018/04/17/why-is-the-kernel-community-replacing-iptables/">https://cilium.io/blog/2018/04/17/why-is-the-kernel-community-replacing-iptables/</a></li>
<li>FB 簡報：<a href="https://www.netdevconf.org/2.1/slides/apr6/zhou-netdev-xdp-2017.pdf">https://www.netdevconf.org/2.1/slides/apr6/zhou-netdev-xdp-2017.pdf</a></li>
</ul>

<h2 id="參考">參考</h2>

<ul>
<li><a href="https://gigenchang.wordpress.com/2014/04/19/10%E5%88%86%E9%90%98%E5%AD%B8%E6%9C%83iptables/">https://gigenchang.wordpress.com/2014/04/19/10%E5%88%86%E9%90%98%E5%AD%B8%E6%9C%83iptables/</a></li>
</ul>

  </div>
  <div>
    <ul class="post__nav">
        <li class="post__nav--next">
            
        </li>
        <li class="post__nav--previous">
            <i class="fas fa-chevron-left"></i>
            <a href="/posts/2018-10-15-hands-on-k8s-kube-proxy-w-ipvs-lb/">                
                <span class="previous__heading">PREVIOUS</span>
                <span class="previous__title">實現 IPVS-based K8s service load balancing - 不同 namespace 擁有自己的 external IP</span>
            </a>
        </li>
    </ul>
</div>

  <div class="post__related">
    
    
    
    <h2>Related Articles</h2>
    <ul class="related-posts">
        
<li>
  <span class="list__title--small">
    <a href="/posts/2018-10-15-hands-on-k8s-kube-proxy-w-ipvs-lb/">實現 IPVS-based K8s service load balancing - 不同 namespace 擁有自己的 external IP</a>
      
      <time class="pull-right hidden-tablet">Oct 15 &#39;18</time>
      
  </span>
</li>

<li>
  <span class="list__title--small">
    <a href="/2018/03/09/kubernetes-%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98/">[Kubernetes] 學習筆記</a>
      
      <time class="pull-right hidden-tablet">Mar 09 &#39;18</time>
      
  </span>
</li>

    </ul>
    
</div>
</article>
        </div>
        <div class="container content"><script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>

<script type="text/javascript">
    hljs.configure({languages: []});
    hljs.initHighlightingOnLoad();
</script>
<div id="disqus_thread"></div>
<script type="text/javascript">
      (function () {
            
            
            
            if (location.hostname === "localhost" || 
            	location.hostname === "127.0.0.1" || 
            	location.hostname === "") {
                return;
			}
            var dsq = document.createElement('script');
            dsq.type = 'text/javascript';
            dsq.async = true;
            var disqus_shortname = 'bestsamina';
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(
                  dsq);
      })();
</script>

<noscript>
	Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


    </div>
    

    </body>
</html>
